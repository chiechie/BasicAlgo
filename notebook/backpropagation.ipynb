{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习--使用链式法则对复合函数球导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x, y)=\\frac{x+\\sigma(y)}{\\sigma(x)+(x+y)^{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 3 # example values\n",
    "y = -4\n",
    "import numpy as np\n",
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## forward\n",
    "a = x + y\n",
    "b = sigmoid(x)\n",
    "c = sigmoid(y)\n",
    "d = x + c\n",
    "e = a * a\n",
    "f = b + e\n",
    "fx =  d / f\n",
    "            \n",
    "## compute local gradients\n",
    "              \n",
    "DaDx = 1\n",
    "DaDy = 1\n",
    "DbDx = b * (1-b)\n",
    "DcDy = c * (1-c)\n",
    "DdDx = 1\n",
    "DdDc = 1\n",
    "DeDa = 2 * a\n",
    "DfDb = 1\n",
    "DfDe = 1\n",
    "DfxDd = 1 / f\n",
    "DfxDf = - d / f**2\n",
    "\n",
    "            \n",
    "## back propagation layer by layer\n",
    "## 需要构建一个计算图，还要做裁剪\n",
    "              \n",
    "DfxDd = 1 / f\n",
    "DfxDf = - d / f**2\n",
    "              \n",
    "                  \n",
    "DdDy = DdDc * DcDy\n",
    "DbDx = b * (1-b)\n",
    "DeDx = DeDa * DaDx\n",
    "DfDx = DfDb * DbDx + DfDe * DeDx\n",
    "\n",
    "DfxDx = DfxDd * DdDx + DfxDf * DfDx \n",
    "DeDy = DeDa * DaDy\n",
    "DfDy = DfDe * DeDy\n",
    "DfxDy = DfxDd * DdDy + DfxDf * DfDy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DfxDy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "w = [2,-3,-3] # assume some random weights and data\n",
    "x = [-1, -2]\n",
    "\n",
    "# forward pass\n",
    "dot = w[0]*x[0] + w[1]*x[1] + w[2]\n",
    "f = 1.0 / (1 + math.exp(-dot)) # sigmoid function\n",
    "\n",
    "# backward pass through the neuron (backpropagation)\n",
    "ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation\n",
    "dx = [w[0] * ddot, w[1] * ddot] # backprop into x\n",
    "dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w\n",
    "# we're done! we have the gradients on the inputs to the circuit\n",
    "dw, ddot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用向后传播算法计算f对于w的梯度：\n",
    "\n",
    "$$\n",
    "f(w, x)=\\frac{1}{1+e^{-\\left(w_{0} x_{0}+w_{1} x_{1}+w_{2}\\right)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x[0] * w[0] +  x[1] * w[1] + w[2]\n",
    "z = sigmoid(y) \n",
    "\n",
    "Dy = z * (1 - z) \n",
    "DyDw = [x[0], x[1], 1]\n",
    "DzDw = [x[0] * z * (1 - z) , x[1]* z * (1 - z), z * (1 - z) ]\n",
    "DzDw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4113945723849875"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z * (1 - z) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
